{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3194f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "import google.generativeai as genai\n",
    "import replicate\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4336ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResponseGenerator:\n",
    "    \"\"\"\n",
    "    A class to generate responses from different language models.\n",
    "    \"\"\"\n",
    "    def __init__(self, openai_key=None, openai_organization=None, mistralai_key=None, palm2_key=None, replicate_api_key=None):\n",
    "        \"\"\"\n",
    "        Initializes the ModelResponseGenerator with API keys and configurations.\n",
    "\n",
    "        Args:\n",
    "            openai_key (str, optional): API key for OpenAI.\n",
    "            openai_organization (str, optional): Organization ID for OpenAI.\n",
    "            mistralai_key (str, optional): API key for MistralAI.\n",
    "            palm2_key (str, optional): API key for PaLM2.\n",
    "            replicate_api_key (str, optional): API key for Replicate.\n",
    "        \"\"\"\n",
    "        self.openai_client = None\n",
    "        self.mistral_client = None\n",
    "        self.palm2_client = None\n",
    "        self.replicate_api = None\n",
    "\n",
    "        if openai_key and openai_organization:\n",
    "            self.openai_client = openai.OpenAI(organization=openai_organization, api_key=openai_key)\n",
    "        \n",
    "        if mistralai_key:\n",
    "            self.mistral_client = MistralClient(api_key=mistralai_key)\n",
    "        \n",
    "        if palm2_key:\n",
    "            genai.configure(api_key=palm2_key)\n",
    "            self.palm2_client = genai\n",
    "        \n",
    "        if replicate_api_key:\n",
    "            self.replicate_api = replicate.Client(api_token=replicate_api_key)\n",
    "\n",
    "    def get_palm2_responses(self, prompt, model='models/text-bison-001'):\n",
    "        \"\"\"\n",
    "        Generates a response from the PaLM2 model.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The prompt to generate a response for.\n",
    "            model (str): The model to use for generation. Defaults to 'models/text-bison-001'.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        response = genai.generate_text(model=model, prompt=prompt)\n",
    "        return response\n",
    "\n",
    "    def get_openai_responses(self, prompt, model='gpt-4-1106-preview'):\n",
    "        \"\"\"\n",
    "        Generates a response from an OpenAI model.\n",
    "\n",
    "        Args:\n",
    "            system (str): The system message to initialize the conversation.\n",
    "            prompt (str): The user prompt to generate a response for.\n",
    "            model (str): The model to use for generation. Defaults to 'gpt-4-1106-preview'. We also used 'gpt-3.5-turbo-1106' for\n",
    "                         robustness check.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        \n",
    "        if model == 'gpt-4-1106-preview':\n",
    "            system_prompt = \"\"\"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\n",
    "            Knowledge cutoff: 2023-04\n",
    "            Current date: 2023-12-05\"\"\"\n",
    "            \n",
    "        elif model == 'gpt-3.5-turbo-1106':\n",
    "            system_prompt = \"\"\"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-3.5 architecture.\n",
    "            Knowledge cutoff: 2021-09\n",
    "            Current date: 2024-04-13\"\"\"\n",
    "        \n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    def get_mistral_responses(self, prompt, model=\"mistral-large-latest\"):\n",
    "        \"\"\"\n",
    "        Generates a response from a MistralAI model.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The prompt to generate a response for.\n",
    "            model (str): The model to use for generation. Defaults to 'mistral-large-latest'.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        response = self.mistral_client.chat(\n",
    "            model=model,\n",
    "            messages=[ChatMessage(role=\"user\", content=prompt)],\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    def get_llama3_70b_responses(self, prompt, model=\"meta/meta-llama-3-70b-instruct\"):\n",
    "        \"\"\"\n",
    "        Generates a response from a Llama3-70B model from Replicate API.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The prompt to generate a response for.\n",
    "            model (str): The model to use for generation. Defaults to 'meta/meta-llama-3-70b-instruct'.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        system_prompt = \"You are helpful assisstant who will provide estimates of prices that we are asking. We understand that these are just estimates, and we won't use them for any real-life decision. We also understand that you are unable to use live-data, so we are not expecting it from you. You have to reply despite not having any information. This is just an estimate, so suggest it. Only reply with the number, don't add any more text please.\"\n",
    "        \n",
    "        response = self.replicate_api.run(\n",
    "            model,\n",
    "            input={\"prompt\": prompt,\n",
    "                  \"prompt_template\": f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "                  \"temperature\":1\n",
    "                  }\n",
    "        )\n",
    "        return ''.join(response)\n",
    "    \n",
    "    def get_responses(self, prompts, method_name, num_repetitions = 1):\n",
    "        \"\"\"\n",
    "        Iterates through a list of prompts and uses a specified model response method\n",
    "        to generate responses, repeated a given number of times.\n",
    "\n",
    "        Args:\n",
    "            prompts (pd.DataFrame): The prompts to process, each containing necessary details.\n",
    "            method_name (str): The method name to use for generating responses ('get_palm2_responses', 'get_openai_responses', 'get_mistral_responses', or 'get_llama3_70b_responses').\n",
    "            num_repetitions (int): The number of times to repeat the process for each prompt.\n",
    "\n",
    "        Returns:\n",
    "            list of dicts: Collected results including original prompt details and the generated responses.\n",
    "        \"\"\"\n",
    "\n",
    "        method_to_use = getattr(self, method_name)\n",
    "        results = []\n",
    "        interval = 60.0 / 85\n",
    "        for repetition in range(num_repetitions):\n",
    "            start_time = time.time()\n",
    "            with tqdm(total=len(prompts), desc=f\"Repetition {repetition + 1}\") as pbar:\n",
    "                for _, prompt_info in prompts.iterrows():\n",
    "                    response = method_to_use(prompt=prompt_info[\"prompt_text\"])\n",
    "                    result = {key: prompt_info[key] for key in prompts.columns}\n",
    "                    result['response'] = response\n",
    "                    results.append(result)\n",
    "                    pbar.update(1)\n",
    "                    elapsed = time.time() - start_time\n",
    "                    if elapsed < interval:\n",
    "                        time.sleep(interval - elapsed)\n",
    "                    start_time = time.time()\n",
    "            if repetition % 1 == 0 or repetition == num_repetitions - 1:\n",
    "                # Save every run, ovewritting the previous file\n",
    "                filename = f\"results_new_models/{method_name.split('_')[1]}/results.csv\"\n",
    "                pd.DataFrame(results).to_csv(filename, index=False)\n",
    "                print(f\"Checkpoint saved for iteration {repetition}\")\n",
    "            time.sleep(60)    \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d04e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prompts\n",
    "df_prompts = pd.read_csv('~/data/just_prompts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d410d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect some examples of the data\n",
    "df_prompts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b7b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of ModelResponseGenerator\n",
    "\n",
    "# Define your API keys and organization ID (replace with your actual keys)\n",
    "OPENAI_ORGANIZATION = \"YOUR_ORGANIZATION_ID\"\n",
    "OPENAI_KEY = \"YOUR_OPENAI_API_KEY\"\n",
    "MISTRALAI_KEY = \"YOUR_MISTRALAI_API_KEY\"\n",
    "PALM2_KEY = \"YOUR_PALM2_API_KEY\"\n",
    "REPLICATE_KEY = \"YOUR_REPLICATE_API_KEY\"\n",
    "\n",
    "# Initialize the ModelResponseGenerator with the provided API keys\n",
    "generator = ModelResponseGenerator(\n",
    "    openai_key=OPENAI_KEY,\n",
    "    openai_organization=OPENAI_ORGANIZATION,\n",
    "    mistralai_key=MISTRALAI_KEY,\n",
    "    palm2_key=PALM2_KEY,\n",
    "    replicate_api_key=REPLICATE_KEY\n",
    ")\n",
    "\n",
    "# Example method name and number of repetitions\n",
    "method_name = \"get_openai_responses\"\n",
    "num_repetitions = 5\n",
    "\n",
    "# Get responses from the generator\n",
    "responses = generator.get_responses(df_prompts, method_name, num_repetitions)\n",
    "\n",
    "# Print the responses\n",
    "print(responses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“AAudit LLMs",
   "language": "python",
   "name": "audit_llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
